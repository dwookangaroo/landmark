{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "15c18aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageChops\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f3ebdb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time, random\n",
    "from tqdm import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5e40cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 꺼내오기 \n",
    "# 독립문 훈련, 테스트셋 나누기 \n",
    "door_folder = \"C:/source/image/indep_door\"\n",
    "park_folder = \"C:/source/image/tapgol_park\"\n",
    "door_file_list = [x for x in os.listdir(door_folder)]\n",
    "park_file_list = [x for x in os.listdir(park_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5032c9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "door_test_list = door_file_list[:200]\n",
    "len(door_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e58097e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "door_train_list = door_file_list[200:]\n",
    "len(door_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "baa46c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 200/200 [00:14<00:00, 14.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# 독립문 테스트셋\n",
    "file_name_freq  = 0\n",
    "file_name_freq += 1 \n",
    "for i in tqdm(range(len(door_test_list))):\n",
    "    img = load_img(\"C:/source/image/indep_door/\" + door_test_list[i])\n",
    "    img = img.resize((300, 300))\n",
    "    img.save(\"C:/source/image/test/indep_door/\" + door_test_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "67797435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 136/136 [00:10<00:00, 12.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# 독립문 훈련셋\n",
    "for i in tqdm(range(len(door_train_list))):\n",
    "    img = load_img(\"C:/source/image/indep_door/\" + door_train_list[i])\n",
    "    img = img.resize((300, 300))\n",
    "    img.save(\"C:/source/image/train/indep_door/\" + door_train_list[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2194e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "park_test_list = park_file_list[:200]\n",
    "park_train_list = park_file_list[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d71f1b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 200/200 [00:19<00:00, 10.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# 탑골공원 테스트셋\n",
    "for i in tqdm(range(len(park_test_list))):\n",
    "    img = load_img(\"C:/source/image/tapgol_park/\" + park_test_list[i])\n",
    "    img = img.resize((300, 300))\n",
    "    img.save(\"C:/source/image/test/tapgol_park/\" + park_test_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1940520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 605/605 [00:57<00:00, 10.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# 탑골공원 훈련셋\n",
    "for i in tqdm(range(len(park_train_list))):\n",
    "    img = load_img(\"C:/source/image/tapgol_park/\" + park_train_list[i])\n",
    "    img = img.resize((300, 300))\n",
    "    img.save(\"C:/source/image/train/tapgol_park/\" + park_train_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ba04916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이미지 부풀리기\n",
    "datagen = ImageDataGenerator(\\\n",
    "                            rotation_range=10, # 정수범위만큼 무작위 회전\n",
    "                            width_shift_range=0.1,  # 픽셀단위로 좌우이동\n",
    "                            height_shift_range=0.1, # 픽셀단위로 상하이동\n",
    "                            shear_range=0.5, # 이미지 찌그러트리기\n",
    "                            zoom_range=0.2, # 이미지 확대 축소 \n",
    "                            horizontal_flip=True, # 좌우반전\n",
    "                            fill_mode=\"nearest\") # 이미지에 여백 생겼을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "003c3fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                     | 0/136 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/source/image/train/indep_door/HF010435_0103_0089.JPG'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-186-ab0a45e7b74c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfile_name_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoor_train_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/source/image/train/indep_door/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdoor_train_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# PIL 이미지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (3, 300, 300) 크기의 Numpy 배열\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (1, 3, 300, 300) 크기의 Numpy 배열\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    298\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msupported\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m   \"\"\"\n\u001b[1;32m--> 300\u001b[1;33m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0m\u001b[0;32m    301\u001b[0m                         target_size=target_size, interpolation=interpolation)\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras_preprocessing\\image\\utils.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[0;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'grayscale'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/source/image/train/indep_door/HF010435_0103_0089.JPG'"
     ]
    }
   ],
   "source": [
    "# 학습이미지만 부풀리기\n",
    "# 독립문 \n",
    "file_name_freq = 0\n",
    "for i in tqdm(range(len(door_train_list))):\n",
    "    img = load_img(\"C:/source/image/train/indep_door/\" + door_train_list[i])  # PIL 이미지\n",
    "    x = img_to_array(img) # (3, 300, 300) 크기의 Numpy 배열\n",
    "    x = x.reshape((1,) + x.shape) # (1, 3, 300, 300) 크기의 Numpy 배열\n",
    "    file_name_freq += 1\n",
    "    \n",
    "    # 변환된 이미지를 배치 단위로 생성해서 지정된 폴더에 저장\n",
    "    i = 0\n",
    "    for batch in datagen.flow(x, batch_size=1,\\\n",
    "                             save_to_dir=\"C:/source/image/train/indep_door/\",\\\n",
    "                             save_prefix=door_train_list[i] + str(file_name_freq),\\\n",
    "                             save_format=\"jpg\"):\n",
    "        i += 1\n",
    "        if i > 6:\n",
    "            break # 이미지 한 개 당 6개씩 부풀리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "16cff81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 학습이미지만 부풀리기\n",
    "# # 탑골공원 \n",
    "# file_name_freq = 0\n",
    "# for i in tqdm(range(len(park_train_list))):\n",
    "#     img = load_img(\"C:/source/image/train/tapgol_park/\" + park_train_list[i])  # PIL 이미지\n",
    "#     x = img_to_array(img) # (3, 300, 300) 크기의 Numpy 배열\n",
    "#     x = x.reshape((1,) + x.shape) # (1, 3, 300, 300) 크기의 Numpy 배열\n",
    "#     file_name_freq += 1\n",
    "    \n",
    "#     # 변환된 이미지를 배치 단위로 생성해서 지정된 폴더에 저장\n",
    "#     i = 0\n",
    "#     for batch in datagen.flow(x, batch_size=1,\\\n",
    "#                              save_to_dir=\"C:/source/image/train/tapgol_park/\",\\\n",
    "#                              save_prefix=park_train_list[i] + str(file_name_freq),\\\n",
    "#                              save_format=\"jpg\"):\n",
    "#         i += 1\n",
    "#         if i > 2:\n",
    "#             break # 이미지 한 개 당 6개씩 부풀리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5c934170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 훈련시키기 \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers, initializers, regularizers, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1b3f25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "11aa63bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory('C:/source/image/train/',target_size=(300,300),\n",
    "                                                    batch_size=b_size, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d8dbb230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory('C:/source/image/test/',target_size=(300,300),\n",
    "                                                    batch_size=b_size, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "100cc93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(15, (3, 3), input_shape=(300,300,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(15, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(30, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(30))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "76836856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 298, 298, 15)      420       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 298, 298, 15)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 149, 149, 15)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 147, 147, 15)      2040      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 147, 147, 15)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 73, 73, 15)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 71, 71, 30)        4080      \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 71, 71, 30)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 35, 35, 30)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 36750)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 30)                1102530   \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 62        \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,109,132\n",
      "Trainable params: 1,109,132\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0280f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(learning_rate=0.0002),\\\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "94602a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_train = len(train_generator)\n",
    "steps_test = len(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3a1ca6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 242s 121ms/step - loss: 0.1016 - accuracy: 0.9525 - val_loss: 0.0621 - val_accuracy: 0.9825\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 246s 123ms/step - loss: 0.0381 - accuracy: 0.9845 - val_loss: 0.1191 - val_accuracy: 0.9850\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 266s 133ms/step - loss: 0.0196 - accuracy: 0.9865 - val_loss: 0.1404 - val_accuracy: 0.9825\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 234s 117ms/step - loss: 0.0124 - accuracy: 0.9940 - val_loss: 0.1793 - val_accuracy: 0.9750\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 209s 104ms/step - loss: 0.0046 - accuracy: 0.9970 - val_loss: 0.2212 - val_accuracy: 0.9775\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 193s 96ms/step - loss: 0.0087 - accuracy: 0.9970 - val_loss: 0.2694 - val_accuracy: 0.9775\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 180s 90ms/step - loss: 0.0022 - accuracy: 0.9985 - val_loss: 0.3611 - val_accuracy: 0.9800\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 179s 89ms/step - loss: 0.0014 - accuracy: 0.9985 - val_loss: 0.3312 - val_accuracy: 0.9825\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 187s 93ms/step - loss: 0.0018 - accuracy: 0.9985 - val_loss: 0.4277 - val_accuracy: 0.9825\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 174s 87ms/step - loss: 0.0048 - accuracy: 0.9980 - val_loss: 0.5658 - val_accuracy: 0.9775\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "       train_generator,\n",
    "       steps_per_epoch=steps_train,\n",
    "       epochs=10,\n",
    "       validation_data=test_generator,\n",
    "       validation_steps=steps_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac628f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
